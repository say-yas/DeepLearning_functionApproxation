# NNA_24_Hammad_Sayyad

This is a package for approximating functions using various deep neural networks.
The codes are implemented by <ins>Muhammad Hammad</ins> and <ins>Sharareh Sayyad</ins>.


## Problem to be addressed

The goal of this project is to approximate a function $f:~[0, 1] \rightarrow \mathbb{R} \text{ or } f:~[0, 1]^{2} \rightarrow \mathbb{R}$ by using a neural network. Deep neural networks (DNNs) have succeeded greatly in areas like image processing, natural language processing, and video and audio synthesis. They have long been used for regression tasks, approximating functions from samples. As universal approximators for continuous functions, DNNs with ReLU activation functions have been shown to provide guaranteed convergence rates for certain function classes; see Refs.[1,2,3,4].

[1] Bo Liu, Yi Liang (2021). Optimal function approximation with ReLU neural networks, Neurocomputing, Volume 435.

[2] Fokina, Daria and Oseledets, Ivan. (2023). Growing axons: greedy learning of neural networks with application to function approximation. Russian Journal of Numerical Analysis and Mathematical Modelling. 38. 1-12. 10.1515/rnam-2023-0001.

[3] Implementation of [Axon algorithm](https://github.com/dashafok/axon-approximation) for function approximation.

[4]  M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations.
Journal of Computational Physics, 378:686â€“707, 2019.
